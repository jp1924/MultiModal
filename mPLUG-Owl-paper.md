# mPLUG-Owl : Modularization Empowers Large Language Models with Multimodality

## Abstract

Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM, while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with frozen LLM module to align the image and text. In the second stage, languageonly and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperform existing multi-modal models, demonstrating mPLUG-Owl’s impressive instruction and visual understanding ability, multi-turn conversation ability and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at <https://github.com/X-PLUG/mPLUG-Owl>. The online demo is available at <https://www.modelscope.cn/studios/damo/mPLUG-Owl>.

대규모 언어 모델(LLM)은 다양한 개방형 과제에서 인상적인 제로 샷 능력을 보여줬으며, 최근 연구에서는 멀티 모달 생성에 LLM을 사용하는 방안도 모색되고 있습니다. 이 연구에서는 기초 LLM, 시각 지식 모듈, 시각 추상화 모듈의 모듈화된 학습을 통해 LLM이 멀티 모달 능력을 갖추도록 하는 새로운 훈련 패러다임인 mPLUG-Owl을 소개합니다. 이 접근 방식은 여러 모달리티를 지원하고 모달리티 간 협업을 통해 다양한 단일 모달리티 및 다중 모달리티 능력을 촉진할 수 있습니다. mPLUG-Owl의 훈련 패러다임은 이미지와 텍스트를 정렬하는 2단계 방법을 통해 LLM의 도움을 받아 시각 지식을 학습하는 동시에 LLM의 생성 능력을 유지 및 개선합니다. 첫 번째 단계에서는 이미지와 텍스트를 정렬하기 위해 시각 지식 모듈과 추상화 모듈을 고정된 LLM 모듈로 훈련합니다. 두 번째 단계에서는 언어 전용 및 다중 모달 지도 데이터 세트를 사용하여 시각 지식 모듈을 동결함으로써 LLM과 추상화 모듈에 대한 로우랭크 적응(LoRA) 모듈을 공동으로 미세 조정합니다. 시각 관련 명령어 평가 세트 OwlEval을 신중하게 구축합니다. 실험 결과, 우리 모델은 기존의 멀티모달 모델보다 뛰어난 성능을 보이며 mPLUG-Owl의 인상적인 지시 및 시각적 이해 능력, 멀티턴 대화 능력, 지식 추론 능력을 입증했습니다. 또한 다중 이미지 상관관계 및 장면 텍스트 이해와 같은 예상치 못한 흥미로운 능력도 관찰되어 시각만으로 문서를 이해하는 것과 같은 더 어려운 실제 시나리오에 활용할 수 있습니다. 코드, 사전 훈련된 모델, 인스트럭션 조정 모델, 평가 세트는 <https://github.com/X-PLUG/mPLUG-Owl> 에서 확인할 수 있습니다. 온라인 데모는 <https://www.modelscope.cn/studios/damo/mPLUG-Owl> 에서 확인할 수 있습니다.

## Introduction

Large language models (LLMs) such as GPT-3 [Brown et al., 2020], BLOOM [Scao et al., 2022], LLaMA [Touvron et al., 2023] have experienced rapid development to make general artificial intelligence possible, which demonstrates impressive zero-shot abilities on various linguistic applications. However, except GPT-4 [OpenAI, 2023], current general LLMs cannot support different modalities of input and develop impressive multimodal abilities. Although GPT-4 [OpenAI, 2023] has exhibited remarkable multimodal abilities, the methods behind its extraordinary abilities remain a mystery. Recently, researchers have been extending LLMs to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. However, systematic collaboration approaches, including Visual ChatGPT [Wu et al., 2023], MM-REACT [Yang et al., 2023], and HuggingGPT [Shen et al., 2023], are designed to facilitate the coordination of various vision models or tools to express visual information with text descriptions. However, these approaches may not be able to comprehend specific multimodal instructions due to their lack of alignment with different modalities. Additionally, these approaches may encounter challenges related to inference efficiency and cost. End-to-end models, such as BLIP-2 [Li et al., 2023], LLaVA [Liu et al., 2023], and MiniGPT-4 [Zhu et al., 2023a] aim to use unified models to support different modalities. However, these models have some limitations as they take frozen visual models, which may lead to inadequate alignment due to the limited number of parameters. Moreover, they cannot unlock various abilities due to missing unimodal and multimodal instruction.

In this paper, we present mPLUG-Owl with an innovative modularized training paradigm for large multi-modal language models that can support multiple modalities concurrently, drawing inspiration from the concept of modularization [Xu et al., 2023b, Li et al., 2022, Xu et al., 2021, Ye et al., 2022]. Our method harnesses the power of pre-trained LLM, visual knowledge module, and connected visual abstractor module to achieve effective alignment between images and text, and utilizes a twostage training scheme to stimulate impressive unimodal and multimodal abilities. Our approach even enhances the strong generation abilities of LLM by modality collaboration between modalities. In the first step, we align the image and text to acquire comprehensive visual knowledge using textimage pairs, which is accomplished by training the visual knowledge module and abstractor module with the frozen LLM module. Subsequently, we fine-tune mPLUG-Owl with language-only and multi-modal instructions to unlock a range of unimodal and multimodal abilities. We freeze the visual knowledge module and train low-rank adaption (LoRA) [Hu et al., 2022] on LLM and visual abstractor module jointly. This approach allows for the effective integration of textual and visual information, facilitating the development of versatile and robust cognitive abilities.

Our experiments on a carefully-built visually related instruction evaluation set OwlEval shows that mPLUG-Owl outperforms existing models such as MiniGPT-4 [Zhu et al., 2023a] and LLaVA [Liu et al., 2023]. We separately verifies mPLUG-Owl’s remarkable abilities in instruction understanding, visual understanding, knowledge transfer, and multi-turn dialogue. Abundant ablation study is performed to show the effectiveness of our training paradigm. Furthermore, we find some unexpected emerging ability such as multi-image correlation, multilingual conversation and scene text understanding.

Our main contributions can be highlighted as follows:
• We propose mPLUG-Owl, a novel training paradigm for large language models through modularization.
• We carefully construct an instruction evaluation set, dubbed OwlEval, to assess the capabilities of different models in the context of visual-related tasks.
• Experimental results demonstrate that mPLUG-Owl excels in multi-modal instruction understanding and multi-turn dialogue, surpassing the performance of existing models.

GPT-3[Brown 외, 2020], BLOOM[Scao 외, 2022], LLaMA[Touvron 외, 2023]와 같은 대규모 언어 모델(LLM)은 다양한 언어 응용 분야에서 인상적인 제로 샷 능력을 보여주는 일반 인공 지능을 가능하게 하는 빠른 발전을 경험했습니다. 그러나 GPT-4 [OpenAI, 2023]를 제외한 현재의 일반 LLM은 다양한 입력 방식을 지원하지 못하며 인상적인 멀티모달 능력을 개발할 수 없습니다. GPT-4[OpenAI, 2023]가 놀라운 멀티모달 능력을 보여주긴 했지만, 그 놀라운 능력의 비결은 여전히 미스터리로 남아 있습니다. 최근 연구자들은 체계적인 협업과 엔드투엔드 훈련 모델이라는 두 가지 패러다임으로 시각적 입력을 이해하기 위해 LLM을 확장하고 있습니다. 그러나 Visual ChatGPT[Wu et al., 2023], MM-REACT[Yang et al., 2023], HuggingGPT[Shen et al., 2023] 등 체계적인 협업 접근법은 시각 정보를 텍스트 설명으로 표현하기 위해 다양한 시각 모델이나 도구를 쉽게 조정하도록 설계되었습니다. 그러나 이러한 접근 방식은 서로 다른 양식에 맞지 않기 때문에 특정 멀티모달 지침을 이해하지 못할 수 있습니다. 또한 이러한 접근 방식은 추론 효율성 및 비용과 관련된 문제에 직면할 수 있습니다. BLIP-2[Li 외, 2023], LLaVA[Liu 외, 2023], MiniGPT-4[Zhu 외, 2023a]와 같은 엔드투엔드 모델은 통합 모델을 사용하여 다양한 모달리티를 지원하는 것을 목표로 합니다. 그러나 이러한 모델은 고정된 시각 모델을 사용하기 때문에 제한된 매개변수 수로 인해 부적절한 정렬을 초래할 수 있는 몇 가지 한계가 있습니다. 또한 단일 모달 및 다중 모달 인스트럭션이 누락되어 다양한 능력을 발휘할 수 없습니다.

이 논문에서는 모듈화 개념에서 영감을 얻어 여러 모달리티를 동시에 지원할 수 있는 대규모 다중 모달 언어 모델을 위한 혁신적인 모듈화된 훈련 패러다임을 갖춘 mPLUG-Owl을 소개합니다[Xu et al., 2023b, Li et al., 2022, Xu et al., 2021, Ye et al., 2022]. 우리의 방법은 사전 훈련된 LLM, 시각 지식 모듈, 연결된 시각 추상화 모듈의 힘을 활용하여 이미지와 텍스트 간의 효과적인 정렬을 달성하고, 인상적인 단일 모드 및 다중 모드 능력을 자극하기 위해 2단계 훈련 체계를 활용합니다. 이러한 접근 방식은 모달리티 간 협업을 통해 LLM의 강력한 생성 능력까지 향상시킵니다. 첫 번째 단계에서는 텍스트이미지 쌍을 사용하여 이미지와 텍스트를 정렬하여 포괄적인 시각 지식을 습득하며, 이는 시각 지식 모듈과 추상화 모듈을 고정된 LLM 모듈로 훈련하여 이루어집니다. 그 후, 언어 전용 및 다중 모드 명령어로 mPLUG-Owl을 미세 조정하여 다양한 단일 모드 및 다중 모드 능력을 잠금 해제합니다. 시각적 지식 모듈을 고정하고 LLM과 시각적 추상화 모듈에 대해 저수준 적응[LoRA](Hu et al., 2022)을 공동으로 훈련합니다. 이러한 접근 방식을 통해 텍스트 정보와 시각 정보를 효과적으로 통합하여 다재다능하고 강력한 인지 능력을 개발할 수 있습니다.

세심하게 구축된 시각적 관련 명령어 평가 세트 OwlEval에 대한 실험 결과, mPLUG-Owl은 MiniGPT-4 [Zhu 외, 2023a] 및 LLaVA [Liu 외, 2023] 같은 기존 모델보다 뛰어난 성능을 보였습니다. 명령 이해, 시각적 이해, 지식 전달 및 다중 턴 대화에서 mPLUG-Owl의 뛰어난 능력을 별도로 검증합니다. 훈련 패러다임의 효과를 보여주기 위해 풍부한 절제 연구를 수행했습니다. 또한 다중 이미지 상관관계, 다국어 대화 및 장면 텍스트 이해와 같은 예상치 못한 새로운 능력도 발견했습니다.

우리의 주요 기여는 다음과 같이 강조할 수 있습니다:

- 모듈화를 통해 대규모 언어 모델을 위한 새로운 훈련 패러다임인 mPLUG-Owl을 제안합니다.
- 시각 관련 작업의 맥락에서 다양한 모델의 기능을 평가하기 위해 OwlEval이라는 명령어 평가 집합을 신중하게 구축합니다.
- 실험 결과에 따르면 mPLUG-Owl은 다중 모드 명령어 이해와 다중 턴 대화에서 기존 모델의 성능을 뛰어넘는 뛰어난 성능을 보여줍니다.

## Related Work

### Large Language Models

In recent times, Large Language Models (LLMs) have garnered increasing attention for their exceptional performance in diverse natural language processing (NLP) tasks. Initially, transformer models such as BERT [Devlin et al., 2019], GPT [Radford and Narasimhan, 2018], and T5 [Raffel et al., 2020] were developed with different pre-training objectives. However, the emergence of GPT3 [Brown et al., 2020], which scales up the number of model parameters and data size, showcases significant zero-shot generalization abilities, enabling them to perform commendably on previously unseen tasks. Consequently, numerous LLMs such as OPT [Zhang et al., 2022], BLOOM [Scao et al., 2022], PaLM [Chowdhery et al., 2022], and LLaMA [Touvron et al., 2023] are created, ushering in the success of LLMs. Additionally, Ouyang et al. [Ouyang et al., 2022] propose InstructGPT by aligning human instruction and feedback with GPT-3. Furthermore, it has been applied to ChatGPT [OpenAI, 2022], which facilitates conversational interaction with humans by responding to a broad range of diverse and intricate queries and instructions.

최근 대규모 언어 모델(LLM)은 다양한 자연어 처리(NLP) 작업에서 뛰어난 성능으로 인해 점점 더 많은 관심을 받고 있습니다. 초기에는 BERT[Devlin et al., 2019], GPT[Radford and Narasimhan, 2018], T5[Raffel et al., 2020]와 같은 트랜스포머 모델이 다양한 사전 학습 목표를 가지고 개발되었습니다. 그러나 모델 파라미터의 수와 데이터 크기를 확장한 GPT3[Brown et al., 2020]의 등장으로 제로 샷 일반화 능력이 크게 향상되어 이전에는 볼 수 없었던 작업에서도 뛰어난 성능을 발휘할 수 있게 되었습니다. 그 결과, OPT [Zhang et al., 2022], BLOOM [Scao et al., 2022], PaLM [Chowdhery et al., 2022], LLaMA [Touvron et al., 2023] 등 수많은 LLM이 탄생하여 LLM의 성공을 이끌고 있습니다. 또한 Ouyang 등은 사람의 지시와 피드백을 GPT-3와 연계하여 InstructGPT를 제안했습니다[Ouyang et al., 2022]. 나아가 다양하고 복잡한 질의와 지시에 광범위하게 응답하여 인간과의 대화형 상호작용을 용이하게 하는 ChatGPT[OpenAI, 2022]에도 적용되었습니다.

### Multi-Modal Large Language Models

Despite the successful applications of LLMs in natural language processing, it is still struggling for LLMs to perceive other modalities such as vision and audio. Recently, researchers have been extending language models to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. Systematic collaboration approaches, such as Visual ChatGPT [Wu et al., 2023], MM-REACT [Yang et al., 2023], and HuggingGPT [Shen et al., 2023], leverage various vision experts or tools to express visual information with text descriptions. Subsequently, large language models, such as ChatGPT, can act as the agents, and be prompted to select the appropriate experts and tools for visual understanding. Finally, LLMs would summarize the output of these experts to answer user queries. On the other hand, some approaches [Li et al., 2023, Alayrac et al., 2022, Liu et al., 2023] leverage the pre-trained large language model to build unified models for multi-modality. For example, Flamingo [Alayrac et al., 2022] freezes the pre-trained vision encoder and large language model and fuses vision and language modalities with gated cross-attention showing impressive few-shot capabilities. Additionally, BLIP-2 [Li et al., 2023] designs Q-Former to align the visual features from the frozen visual encoder and large language models with Flan-T5 [Chung et al., 2022] and OPT [Zhang et al., 2022]. Moreover, PaLM-E [Driess et al., 2023] directly inputs features from sensor modalities with PaLM [Chowdhery et al., 2022], which has 520 billion parameters, contributing to robust performance in real-world perceptions. Furthermore, some powerful instruction-tuned language models that built upon open-sourced foundation model LLaMA [Touvron et al., 2023], such as Alpaca [Taori et al., 2023] and Vicuna [Vicuna, 2023], exhibit comparable performance to ChatGPT [OpenAI, 2022] and GPT-4 [OpenAI, 2023]. MiniGPT-4 [Zhu et al., 2023a] and LLaVA [Liu et al., 2023] align these finetuned models with extracted visual features from the frozen visual backbone. In contrast, mPLUG-Owl not only aligns the representation between the vision and language foundation model (e.g. CLIP and LLaMA) in terms of knowledge acquisition and grounding to the real world but also can understand language and multi-modal instructions, showcasing strong zero-shot generalization and multi-turn conversation capabilities.

자연어 처리에 LLM을 성공적으로 적용했음에도 불구하고, 시각 및 오디오와 같은 다른 양식을 인식하는 데는 여전히 어려움을 겪고 있습니다. 최근 연구자들은 체계적인 협업과 엔드투엔드 학습 모델이라는 두 가지 패러다임으로 시각적 입력을 이해하도록 언어 모델을 확장하고 있습니다. Visual ChatGPT[Wu et al., 2023], MM-REACT[Yang et al., 2023], HuggingGPT[Shen et al., 2023] 등 체계적인 협업 접근 방식은 다양한 시각 전문가 또는 도구를 활용하여 시각 정보를 텍스트 설명으로 표현합니다. 이후 ChatGPT와 같은 대규모 언어 모델이 에이전트 역할을 수행하며 시각적 이해를 위해 적절한 전문가와 도구를 선택하라는 메시지를 표시할 수 있습니다. 마지막으로 LLM은 이러한 전문가의 결과를 요약하여 사용자 쿼리에 답변합니다. 반면에 일부 접근 방식[Li 외, 2023, Alayrac 외, 2022, Liu 외, 2023]은 사전 학습된 대규모 언어 모델을 활용하여 다중 모달리티를 위한 통합 모델을 구축합니다. 예를 들어 플라밍고[Alayrac 외, 2022]는 사전 학습된 비전 인코더와 대규모 언어 모델을 고정하고 비전 및 언어 모달리티를 게이트 교차 주의와 융합하여 인상적인 소수 샷 기능을 보여줍니다. 또한 BLIP-2 [Li et al., 2023]는 고정 시각 인코더와 대규모 언어 모델의 시각적 특징을 Flan-T5 [Chung et al., 2022] 및 OPT [Zhang et al., 2022]와 정렬하기 위해 Q-Former를 설계합니다. 또한 PaLM-E[Driess et al., 2023]는 5,200억 개의 파라미터를 가진 PaLM[Chowdhery et al., 2022]으로 센서 모달리티의 특징을 직접 입력하여 실제 지각에서 강력한 성능을 발휘합니다. 또한, 오픈 소스 기반 모델인 LLaMA[Touvron 외, 2023]를 기반으로 구축된 강력한 명령어 조정 언어 모델인 알파카[Taori 외, 2023], 비쿠나[Vicuna, 2023]는 ChatGPT[OpenAI, 2022] 및 GPT-4[OpenAI, 2023]와 비슷한 성능을 보여줍니다. MiniGPT-4[Zhu et al., 2023a]와 LLaVA[Liu et al., 2023]는 이러한 미세 조정된 모델을 고정된 시각적 백본에서 추출한 시각적 특징과 정렬합니다. 반면, mPLUG-Owl은 지식 습득과 실제 세계에 대한 근거 측면에서 시각과 언어 기반 모델(예: CLIP 및 LLaMA) 간의 표현을 정렬할 뿐만 아니라 언어 및 다중 모드 명령을 이해할 수 있어 강력한 제로 샷 일반화 및 다중 턴 대화 기능을 보여줍니다.

## mPLUG-Owl

### Architecture Overview

As illustrated in Figure 1, there exist mainly three types of end-to-end multimodal LLMs: 1) models that utilize limited parameters with frozen LLM and visual models during pretraining and instruction tuning, such as MiniGPT4; 2) models that incorporate trainable LLMs and frozen visual models, exemplified by Kosmos-1; and 3) models that involve trainable LLMs during instruction tuning and frozen visual models, as seen in LLaVA. Nevertheless, these models exhibit certain constraints since they depend on frozen visual models, which can lead to insufficient alignment due to the limited number of parameters. Furthermore, they fail to effectively stimulate a diverse set of abilities, as they lack both unimodal and multimodal instruction.

To this end, we propose mPLUG-Owl, a multi-modal language model that is capable of perceiving various modalities while taking the visual context and information into account and generating corresponding outputs. Specifically, as illustrated in Figure 2, mPLUG-Owl consists of a vision foundation model fV to encode the visual knowledge, a language foundation model fL, and a visual abstractor module fK. We first obtain dense image representations from the pre-trained visual foundation model fV. However, such dense features would fragment the fine-grained image information and bring large computation due to the lengthy sequence when feeding into fL. To mitigate this issue, we employ the visual abstractor module fK to summarize visual information within several learnable tokens, thereby obtaining higher semantic visual representations and reducing computation, as illustrated in Figure 2. The visual representations are combined with text queries and fed into the language model to generate the response.

그림 1에서 볼 수 있듯이 엔드투엔드 멀티모달 LLM에는 크게 세 가지 유형이 있습니다: 1) MiniGPT4와 같이 사전 훈련 및 인스트럭션 튜닝 중에 제한된 파라미터와 고정된 LLM 및 시각 모델을 활용하는 모델, 2) Kosmos-1에서 예시된 것처럼 훈련 가능한 LLM과 고정된 시각 모델을 통합하는 모델, 3) LLaVA에서 볼 수 있는 것처럼 인스트럭션 튜닝 중에 훈련 가능한 LLM과 고정된 시각 모델을 포함하는 모델입니다. 하지만 이러한 모델은 고정 시각 모델에 의존하기 때문에 제한된 파라미터 수로 인해 불충분한 정렬을 초래할 수 있는 특정 제약이 있습니다. 또한, 단일 모드 및 다중 모드 교육이 모두 부족하기 때문에 다양한 능력을 효과적으로 자극하지 못합니다.

이를 위해 시각적 맥락과 정보를 고려하면서 다양한 양식을 인식하고 그에 맞는 출력을 생성할 수 있는 멀티 모달 언어 모델인 mPLUG-Owl을 제안합니다. 구체적으로 그림 2에서 볼 수 있듯이 mPLUG-Owl은 시각 지식을 인코딩하는 비전 기반 모델 fV, 언어 기반 모델 fL, 시각 추상화 모듈 fK로 구성됩니다. 먼저 사전 학습된 시각 기반 모델 fV에서 밀도가 높은 이미지 표현을 얻습니다. 그러나 이러한 고밀도 특징은 세분화된 이미지 정보를 조각화하고 fL에 입력할 때 긴 시퀀스로 인해 계산량이 커집니다. 이 문제를 완화하기 위해, 그림 2와 같이 시각적 추상화 모듈 fK를 사용해 여러 학습 가능한 토큰 내에서 시각 정보를 요약함으로써 더 높은 의미의 시각적 표현을 얻고 계산을 줄입니다. 시각적 표현은 텍스트 쿼리와 결합되어 언어 모델에 공급되어 응답을 생성합니다.

### Training Scheme

Multimodal Pretraining
Large-scale language models, such as GPT-3 [Brown et al., 2020] and LLaMA [Touvron et al., 2023], are trained on extensive and diverse data collected from the internet, providing them with a comprehensive understanding of the world. This vast knowledge base endows these models with remarkable capabilities across a range of tasks. However, the utilization of visual information in such models remains underexplored. Previous approaches [Zhu et al., 2023a, Liu et al., 2023] have employed a limited number of additional parameters to learn the alignment between visual data and language models, constraining their capacity to comprehend complex visual information. To enhance the ability of large-scale language models to perceive visual information while integrating their internal abilities, we propose a novel training paradigm that incorporates a trainable visual backbone fV and an additional visual abstractor fK, while maintaining the pretrained language model fL in a frozen state. This approach enables the model to effectively capture both low-level and higher semantic visual information and align it with the pre-trained language model without compromising its performance.

Joint Instruction Tuning
Upon completion of the prior phase, the model acquires the ability to retain a considerable amount of knowledge and provide reasonable answers to human queries. Nonetheless, it continues to exhibit challenges in generating coherent linguistic responses. As posited in GPT-3 [Brown et al., 2020], refining the model through instruction tuning is essential for accurately discerning user intentions. Previous attempts [Li et al., 2022, Xu et al., 2023b] in multimodal learning have demonstrated that joint learning from uni-modal and multi-modal sources can lead to significant improvements owing to the collaboration between different modalities. Building on this insight, we present a novel vision-language joint instruction tuning strategy to facilitate better alignment between mPLUG-Owl and human instructions and intentions. Specifically, given that the model can comprehend the visual concepts and knowledge depicted in images through visual knowledge learning, we freeze the entire model and employ low-rank adaption (i.e., LoRA [Huet al., 2022]) to adapt fL by training multiple low-rank matrices for efficient alignment with human instructions. For each data record, we unified them in a snippet of conversation following Vicuna [Vicuna, 2023], and we compute the loss on the response. During the training, we accumulate the gradient for text-only instruction data and multi-modal instruction data for multiple batches and updated the parameters. Therefore, by joint training with both language and multi-modal instructions, mPLUG-Owl can better understand a wide range of instructions and respond with more natural and reliable output. Moreover, our approach can easily handle various text and multi-modal instructions without the need for realignment of the vision and language models, as required by methods such as MiniGPT-4 [Zhu et al., 2023a] and LLaVA [Liu et al., 2023].

Training Objective
The model is trained using the language modeling task, which entails learning to generate subsequent tokens based on the preceding context. The primary objective of the training process is to maximize the log-likelihood of the tokens. It is important to note that only discrete tokens, such as text tokens, are considered in the calculation of the training loss. Most significantly, the emergence of diverse capabilities resulting from the training task during the joint instruction tuning stage enhances the performance of mPLUG-Owl in downstream applications.

Multimodal Pretraining
GPT-3[Brown et al., 2020] 및 LLaMA[Touvron et al., 2023]와 같은 대규모 언어 모델은 인터넷에서 수집한 광범위하고 다양한 데이터를 학습하여 세계에 대한 포괄적인 이해를 제공합니다. 이 방대한 지식 기반은 이러한 모델에 다양한 작업에서 놀라운 능력을 부여합니다. 그러나 이러한 모델에서 시각적 정보의 활용은 아직 충분히 연구되지 않은 상태입니다. 이전의 접근 방식[Zhu 외, 2023a, Liu 외, 2023]은 시각 데이터와 언어 모델 간의 정렬을 학습하기 위해 제한된 수의 추가 파라미터를 사용했기 때문에 복잡한 시각 정보를 이해하는 데 제약이 있었습니다. 대규모 언어 모델의 내부 능력을 통합하면서 시각 정보를 인식하는 능력을 향상시키기 위해, 우리는 사전 학습된 언어 모델 fL을 고정 상태로 유지하면서 학습 가능한 시각 백본 fV와 추가 시각 추상화기 fK를 통합하는 새로운 학습 패러다임을 제안합니다. 이 접근 방식을 통해 모델은 저수준 및 상위 의미의 시각 정보를 효과적으로 캡처하고 성능 저하 없이 사전 학습된 언어 모델에 맞게 조정할 수 있습니다.

Joint Instruction Tuning
이전 단계가 완료되면 모델은 상당한 양의 지식을 보유하고 사람의 질문에 합리적인 답변을 제공할 수 있는 능력을 갖추게 됩니다. 그럼에도 불구하고 일관된 언어적 반응을 생성하는 데는 여전히 어려움을 보입니다. GPT-3[Brown et al., 2020]에서 제시된 바와 같이 사용자의 의도를 정확하게 파악하기 위해서는 명령어 튜닝을 통해 모델을 개선하는 것이 필수적입니다. 멀티모달 학습에 대한 이전의 시도[Li et al., 2022, Xu et al., 2023b]는 단일 모달과 멀티 모달 소스의 공동 학습이 서로 다른 모달 간의 협업으로 인해 상당한 개선을 가져올 수 있음을 입증했습니다. 이러한 인사이트를 바탕으로 mPLUG-Owl과 인간의 지시 및 의도를 더 잘 일치시킬 수 있는 새로운 시각 언어 공동 지시 튜닝 전략을 제시합니다. 구체적으로, 모델이 시각 지식 학습을 통해 이미지에 묘사된 시각적 개념과 지식을 이해할 수 있다는 점을 감안하여, 전체 모델을 동결하고 저순위 적응(즉, LoRA [Huet al., 2022])을 사용하여 인간의 지시와 효율적으로 정렬하기 위해 여러 저순위 행렬을 훈련함으로써 fL을 적응시킵니다. 각 데이터 레코드에 대해 Vicuna [Vicuna, 2023]에 따라 대화 스니펫으로 통합하고 응답에 대한 손실을 계산합니다. 훈련하는 동안 텍스트 전용 명령어 데이터와 다중 모드 명령어 데이터에 대한 기울기를 여러 배치로 축적하고 매개 변수를 업데이트합니다. 따라서 언어 및 멀티모달 명령어에 대한 공동 훈련을 통해 mPLUG-Owl은 광범위한 명령어를 더 잘 이해하고 보다 자연스럽고 안정적인 출력으로 응답할 수 있습니다. 또한, 우리의 접근 방식은 MiniGPT-4 [Zhu et al., 2023a] 및 LLaVA [Liu et al., 2023]와 같은 방법에서 요구하는 것처럼 비전 및 언어 모델을 재조정할 필요 없이 다양한 텍스트 및 멀티모달 명령을 쉽게 처리할 수 있습니다.

Training Objective
모델은 언어 모델링 작업을 사용하여 훈련되며, 이 작업에는 이전 컨텍스트를 기반으로 후속 토큰을 생성하는 학습이 수반됩니다. 훈련 과정의 주요 목표는 토큰의 로그 유사도를 극대화하는 것입니다. 텍스트 토큰과 같은 불연속 토큰만 훈련 손실 계산에 고려된다는 점에 유의해야 합니다. 가장 중요한 것은 공동 명령어 튜닝 단계에서 훈련 작업으로 인한 다양한 기능의 출현으로 다운스트림 애플리케이션에서 mPLUG-Owl의 성능이 향상된다는 점입니다.

## Experiment

### Experimental Setup

#### Model Settings

We choose ViT-L/14 [Dosovitskiy et al., 2021] as the visual foundation model fV which has 24 layers with hidden dimension set as 1024 and patch size set as 14. For faster convergence, the ViT is initialized from CLIP ViT-L/14 model pre-trained via contrastive learning. Different with LLaVA [Liu et al., 2023] and MiniGPT-4 [Zhu et al., 2023a], to demonstrate the effectiveness and generalization ability, we utilize raw LLaMA-7B [Touvron et al., 2023] rather than its instruction-tuned variants such as Alpaca [Taori et al., 2023] and Vicuna [Vicuna, 2023]. The total number of parameters of mPLUG-Owl is about 7.2B. More details about hyper-parameters can be found in Appendix.

숨겨진 치수가 1024로 설정되고 패치 크기가 14로 설정된 24개의 레이어가 있는 시각적 기반 모델 fV로 ViT-L/14 [Dosovitskiy et al., 2021]를 선택했습니다. 보다 빠른 융합을 위해 ViT는 대조 학습을 통해 사전 훈련된 CLIP ViT-L/14 모델에서 초기화됩니다. 효과와 일반화 능력을 입증하기 위해 LLaVA [Liu et al., 2023] 및 MiniGPT-4 [Zhu et al., 2023a]와는 달리, Alpaca [Taori et al., 2023] 및 Vicuna [Vicuna, 2023] 등 명령어로 조정된 변형이 아닌 원시 LLaMA-7B [Touvron et al., 2023]를 활용합니다. mPLUG-Owl의 총 파라미터 수는 약 7.2B입니다. 하이퍼 파라미터에 대한 자세한 내용은 부록에서 확인할 수 있습니다.

#### Data and Training Details

For the first stage, we utilize the image-caption pairs from several datasets, including LAION-400M [Schuhmann et al., 2021], COYO-700M [Byeon et al., 2022], Conceptual Captions [Sharma et al., 2018] and MSCOCO [Chen et al., 2015]. We use a batch size of 2.1 million tokens and train mPLUG-Owl for 50k steps, corresponding to about 104 billion tokens. We adopt the AdamW optimizer with β = (0.9, 0.98), and set the learning rate and weight decay to 0.0001 and 0.1 respectively. We warm up the training with 2k warm-up steps then decay the learning rate with the cosine schedule. The input image is randomly resized to 224 × 224. Besides, we tokenize the text input with SentencePiece [Kudo and Richardson, 2018] tokenizer. For the second stage, we gather pure text instruction data from three distinct sources: 102k data from the Alpaca [Taori et al., 2023], 90k from the Vicuna [Vicuna, 2023], and 50k from the Baize [Xu et al., 2023a]. Additionally, we utilize 150k multi-modal instruction data from the LLaVA dataset [Liu et al., 2023]. We train mPLUG-Owl for 2k steps with the batch size 256, and the learning rate is set to 0.00002.

Baselines. We compare our mPLUG-Owl with end-to-end models and systematic collaboration approaches as follows:

• OpenFlamingo [Zhu et al., 2023b] is an open-source version of Flamingo [Alayrac et al., 2022] model. We use the released code of OpenFlamingo-9B3 to run zero-shot generation.

• BLIP-2 [Li et al., 2023] is pre-trained through bootstrapped learning from off-the-shelf frozen pre-trained image models and large language models using an efficient pre-training strategy. We use the released code of BLIP-2 ViT-G FlanT5XXL to perform zero-shot generation.

• MiniGPT-4 [Zhu et al., 2023a] utilizes a single projection layer to align visual information from a pre-trained vision encoder with LLM. Specifically, they employ the same visual encoder as used in BLIP-2, a ViT coupled with their pre-trained Q-Former, and Vicuna as LLM. We use the released demonstration to perform image-instruction generation.

• LLaVA [Liu et al., 2023] applies a single projection layer to convert image features from pre-trained CLIP visual encoder ViT-L/14 into the language embedding space of Vicuna. We use their released demonstration6 to perform image-instruction generation.

• MM-REACT [Yang et al., 2023] integrates ChatGPT/GPT-4 with various specialized vision experts to achieve multimodal reasoning and action. We use their released demonstration7 to get responses

첫 번째 단계에서는 LAION-400M [Schuhmann 외, 2021], COYO-700M [Byeon 외, 2022], 개념적 캡션 [Sharma 외, 2018], MSCOCO [Chen 외, 2015] 등 여러 데이터 세트의 이미지-캡션 쌍을 활용합니다. 210만 토큰의 배치 크기를 사용하고 약 104억 개의 토큰에 해당하는 5만 스텝에 대해 mPLUG-Owl을 훈련시킵니다. β = (0.9, 0.98)인 AdamW 옵티마이저를 채택하고 학습 속도와 가중치 감쇠를 각각 0.0001과 0.1로 설정합니다. 2k 워밍업 단계로 학습을 예열한 다음 코사인 스케줄로 학습 속도를 감쇠시킵니다. 입력 이미지의 크기는 224 × 224로 임의로 조정됩니다. 또한 SentencePiece [Kudo and Richardson, 2018] 토큰화 도구로 텍스트 입력을 토큰화합니다. 두 번째 단계에서는 세 가지 소스에서 순수한 텍스트 명령어 데이터를 수집합니다: 알파카[Taori et al., 2023]에서 102만 개의 데이터, 비쿠나[Vicuna, 2023]에서 9만 개, 바이즈[Xu et al., 2023a]에서 5만 개를 수집합니다. 또한 LLaVA 데이터 세트의 150만 개의 멀티모달 명령어 데이터를 활용합니다[Liu et al., 2023]. 배치 크기 256으로 2k 단계에 대해 mPLUG-Owl을 훈련하고 학습률은 0.00002로 설정합니다.

기준선. mPLUG-Owl과 엔드투엔드 모델 및 체계적인 협업 접근 방식을 다음과 같이 비교합니다:

- 오픈 플라밍고[Zhu 외, 2023b]는 플라밍고[Alayrac 외, 2022] 모델의 오픈 소스 버전입니다. 우리는 제로 샷 생성을 실행하기 위해 OpenFlamingo-9B3의 릴리스된 코드를 사용합니다.

- BLIP-2[Li et al., 2023]는 효율적인 사전 훈련 전략을 사용하여 기성품으로 미리 훈련된 고정 이미지 모델과 대규모 언어 모델로부터 부트스트랩 학습을 통해 사전 훈련됩니다. 제로 샷 생성을 수행하기 위해 BLIP-2 ViT-G FlanT5XXL의 공개된 코드를 사용합니다.

- MiniGPT-4[Zhu et al., 2023a]는 단일 투영 레이어를 사용하여 사전 훈련된 비전 인코더의 시각 정보를 LLM과 정렬합니다. 특히 BLIP-2에서 사용된 것과 동일한 시각 인코더, 사전 훈련된 Q-Former와 결합된 ViT, 그리고 Vicuna를 LLM으로 사용합니다. 공개된 데모를 사용하여 이미지 인스트럭션 생성을 수행합니다.

- LLaVA[Liu et al., 2023]는 단일 투영 레이어를 적용하여 사전 학습된 CLIP 시각 인코더 ViT-L/14의 이미지 특징을 Vicuna의 언어 임베딩 공간으로 변환합니다. 공개된 데모6를 사용하여 이미지 인스트럭션 생성을 수행합니다.
  
- MM-REACT [Yang et al., 2023]는 다양한 전문 비전 전문가와 ChatGPT/GPT-4를 통합하여 멀티모달 추론과 행동을 달성합니다. 공개된 데모7를 사용하여 다음과 같은 응답을 얻습니다.

### Quantitative analysis

In order to comprehensively evaluate various models, we construct a visually-related evaluation set OwlEval by collecting 82 artificially constructed questions based on 50 images, where 21 from MiniGPT-4, 13 from MM-REACT, 9 from BLIP-2, 3 from GPT-4 and 4 collected by us. Partial images have multiple rounds of questions, refers to multi-turn conversation cases. These questions examine a variety of model capabilities including natural image understanding, diagram and flowchart comprehension, optical character recognition (OCR), multi-modal creation, knowledge-intensive QA, and referential interaction QA. As questions are open-ended, we employ manual evaluation metrics to rate the model’s responses as A, B, C, or D following the rating method proposed in Self-Instruct [Wang et al., 2022].

We manually score 82 responses given by mPLUG-Owl and baselines. The comparison results are shown in Figure 3. First, mPLUG-Owl gets 66 A and B, while the most competitive baseline MiniGPT-4 gets 54. Second, mPLUG-Owl doesn’t get any D scores, outperforming all the models. These results suggest that mPLUG-Owl can better understand both instructions and images, which results in a stronger capability in generating satisfactory responses. For a fair comparison, we have excluded those cases in which MM-REACT failed to make predictions. The results are shown separately in Figure 15 and mPLUG-Owl still exhibits superior performance.

To separately examine the single-turn and multi-turn conversation capabilities, we reorganize 82 questions into a single-turn conversation set and a multi-turn conversation set. The former contains the first question from 50 images. The latter contains 52 questions from multi-turn conversation cases. As shown in Figure 4, the mPLUG-Owl achieves outstanding performance in both singleturn and multi-turn conversations.

### Ablation Study

We ablate the two-stage training scheme and the data modality of instruction tuning. Six dimensions of abilities are defined to complete visually related tasks, as shown in Table 1. For each question, we manually label the required abilities and annotate which abilities are reflected in the model’s response. Table 2 shows the ability accuracy of different variants of mPLUG-Owl.

Training Strategy Ablation. As shown in Table 2, without joint instruction tuning, the model is not good at instruction understanding and fail to generalize pre-training abilities to other tasks (r1 vs r5). With the instruction tuning alone, although the model can better comprehend instructions, the model is incapable of achieving promising performance in visual knowledge-related tasks due to lacking of visually-related knowledge pretraining (r2 vs r5). With both multimodal pretraining and joint instruction tuning, the model achieves the best performance and demonstrates the effectiveness of our two-stage training scheme.

Instruction Data Ablation. By comparing r3 with r4, text-only instruction tuning brings more improvement in instruction understanding, while multi-modal instruction tuning achieves better knowledge and reasoning capabilities. This is due to that visual question answering mainly requires the alignment of vision and language knowledge, which is not optimized during text-only instruction tuning. Besides, we also verify that introducing multi-modal data during instruction tuning could further improve the model’s performance on text-only tasks, as shown in Table 3 (r5 vs r4). Concretely, following the evaluation setting as Vicuna[Vicuna, 2023], for each question, we pair the response of each model with the one given by ChatGPT and prompt ChatGPT8 to give two scores respectively for these two responses. Table 3 shows the total score and the score ratio with the ChatGPT score as a reference.

2단계 교육 체계와 교육 튜닝의 데이터 양식을 폐지합니다. 표 1과 같이 시각적으로 관련된 작업을 완료하기 위해 6가지 능력 차원이 정의되어 있습니다. 각 질문에 대해 필요한 능력에 수동으로 레이블을 지정하고 어떤 능력이 모델의 응답에 반영되는지 주석을 달았습니다. 표 2는 mPLUG-Owl의 다양한 변종에 대한 능력 정확도를 보여줍니다.

훈련 전략 제거. 표 2에서 볼 수 있듯이, 공동 명령어 튜닝이 없는 경우 모델은 명령어 이해 능력이 떨어지고 사전 훈련 능력을 다른 과제에 일반화하지 못합니다(r1 대 r5). 명령어 튜닝만 사용하면 모델은 명령어를 더 잘 이해할 수 있지만 시각 관련 지식 사전 학습이 부족하여 시각 지식 관련 과제에서 유망한 성능을 달성할 수 없습니다(r2 vs r5). 멀티모달 사전 훈련과 공동 명령어 튜닝을 통해 이 모델은 최고의 성능을 달성하고 2단계 훈련 체계의 효과를 입증했습니다.

인스트럭션 데이터 제거. R3와 R4를 비교하면 텍스트 전용 인스트럭션 튜닝은 인스트럭션 이해도를 더 향상시키는 반면, 멀티모달 인스트럭션 튜닝은 지식과 추론 능력을 더 향상시킵니다. 이는 시각적 질문에 답하기 위해서는 주로 시각과 언어 지식의 조화가 필요한데, 텍스트 전용 인스트럭션 튜닝에서는 최적화되지 않기 때문입니다. 또한, 표 3(r5 대 r4)에서 볼 수 있듯이 인스트럭션 튜닝 시 멀티 모달 데이터를 도입하면 텍스트 전용 과제에서 모델의 성능이 더욱 향상될 수 있음을 확인했습니다. 구체적으로, 평가 설정을 Vicuna[Vicuna, 2023]와 동일하게 하여 각 질문에 대해 각 모델의 응답을 ChatGPT에서 제공하는 응답과 쌍을 이루고 ChatGPT8이 이 두 응답에 대해 각각 두 개의 점수를 부여하도록 유도합니다. 표 3은 총 점수와 점수 비율을 ChatGPT 점수를 기준으로 보여줍니다.

### Qualitative Analysis

Knowledge-intensive QA As shown in Figure 5, the instruction expects the model to identify the movie characters in the image. MM-REACT is unable to provide an effective response to the instruction, while MiniGPT-4 understands the instruction but failed to answer the movie characters. In contrast, mPLUG-Owl answers four out of the five characters present in the image. This demonstrates that mPLUG-Owl has a better understanding of the knowledge in the image.

Multi-round Conversation The instruction in Figure 6 requires the model to identify the content of the image based on the referential information. The baseline models often made mistakes when faced with referential expressions related to spatial orientation, human behavior, and target attributes in the questions, whereas mPLUG-Owl provided the most accurate response. This capability stems from mPLUG-Owl’s fine-grained understanding of the image, allowing it to locate the corresponding part of the image based on the referential information in the instruction.

Reasoning Figure 7 shows an instruction asking models to give a prediction based on visual information and explain the reason. mPLUG-Owl analyzes the characteristics of the two teams from the aspects of the lineup and tactics and uses them to reason for the outcome. Although MiniGPT-4 also performs well, its persuasiveness in reasoning is slightly inferior to mPLUG-Owl.

Joke Comprehension The case in Figure 8 comes from the GPT-4[OpenAI, 2023], which requires the model to understand and explain a visually related joke. GPT-4 not only follows the instructions in performing analysis panel by panel but also almost perfectly understands the humor of the charging method. mPLUG-Owl also understands this unusual humor, but it incorrectly identified the “VGA” to “USB”. This is mainly due to the limitation of visual information in our training data. More cases about joke comprehension are shown in Figure 9.
