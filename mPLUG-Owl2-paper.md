# mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration

## Abstract

Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-ofthe-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.

다중 모드 대규모 언어 모델(MLLM)은 다양한 개방형 작업에서 인상적인 인스트럭션 능력을 보여주었습니다. 그러나 이전의 방법은 주로 다중 모달 기능을 향상시키는 데 중점을 두었습니다. 이번 연구에서는 다목적 멀티 모달 대형 언어 모델인 mPLUG-Owl2를 소개하며, 이는 모달 협업을 효과적으로 활용하여 텍스트 및 멀티 모달 작업 모두에서 성능을 향상시킵니다. mPLUG-Owl2는 모듈화된 네트워크 설계를 활용하며 언어 디코더가 다양한 모달을 관리하기 위한 범용 인터페이스 역할을 수행합니다. 특히 mPLUG-Owl2는 공유 기능 모듈을 통합하여 모달리티 협업을 용이하게 하고 모달리티별 기능을 보존하는 모달리티 적응형 모듈을 도입했습니다. 광범위한 실험을 통해 mPLUG-Owl2는 텍스트 작업과 멀티 모달 작업을 모두 일반화할 수 있으며 단일 일반 모델로 최첨단 성능을 달성할 수 있음이 밝혀졌습니다. 특히 mPLUG-Owl2는 순수 텍스트 및 멀티모달 시나리오 모두에서 모달리티 협업 현상을 입증한 최초의 MLLM 모델로, 향후 멀티모달 기반 모델 개발의 선구적인 길을 제시합니다.

## Introduction

Large Language Models (LLMs) such as GPT-3 [6], LLaMA [57, 58], and GPT-4 [46] have garnered significant attention due to their exceptional generalization abilities in text understanding and generation. To facilitate the visionlanguage applications, GPT-4V1 [45] has recently demonstrated impressive multi-modal capabilities in diverse tasks, e.g., description , question answering, etc., sparking interest among researchers in the potential convergence of the vision-language field. This has led to the emergence of a group of Multi-modal Large Language Models (MLLMs) [5, 15, 31, 38, 65, 66, 68, 75], which aim to enhance LLMs with the ability to understand and handle visual problems.

Previous studies [27, 63] in multi-modal learning suggest that different modalities can effectively collaborate, thereby enhancing the performance of both text and multi-modal tasks simultaneously. However, MLLMs is a unified model that supports different modalities and tasks without finetuning for specific tasks. Recent works utilize cross-modal alignment modules (e.g., Q-former [15, 31, 75] and linear layer [10, 38]) to map visual features from the vision encoder into the frozen LLMs to carry out multi-modal tasks by leveraging preserved language capabilities. This strategy, unfortunately, restricts the potential of modality collaboration. As a result, some researchers [38, 68] opt to fine-tune LLMs during multi-modal instruction tuning. While fine-tuning significantly improves multi-modal tasks, it risks weakening text task performance [16]. As illustrated in Figure 1, the challenge of modality collaboration in MLLMs is from applying a single module to balance the gain of modality collaboration and modality interference, where modalities may interfere with each other on a large number of instruction datasets across multiple modalities.

To mitigate this challenge, we present a new generalpurpose multi-modal foundation model, mPLUG-Owl2, in this work. Our model features a modularized network design that takes both modality collaboration and modality interference into account, using the language decoder as a universal interface for managing multi-modal signals. Specifically, mPLUG-Owl2 incorporates certain shared functional modules to promote modality collaboration and introduces a modality-adaptive module that serves as a pivot across different modalities. Therefore, vision and language modalities are projected into a shared semantic space for crossmodality interaction, while the proposed module helps preserve modality-specific features. With our novel architecture, modalities with varying information densities are shielded from modality interference due to the modalityadaptive module and can collaborate effectively in capturing shared information. Furthermore, we introduce an innovative two-stage training paradigm that consists of visionlanguage pre-training and joint vision-language instruction tuning. This paradigm trains the vision encoder across two stages, enabling it to capture both low-level and high-level semantic visual information more effectively.

Extensive experiments illustrate the effectiveness and generalization abilities of mPLUG-Owl2, which achieves state-of-the-art performance on 8 classic vision-language benchmarks using a single generic model. Furthermore, it either first or second in performance on 5 recent zeroshot multi-modal benchmarks, underscoring its adaptability and proficiency in multi-modal instruction comprehension and generation. In addition to its cutting-edge performance in multi-modal tasks, mPLUG-Owl2 also achieves state-ofthe-art results on multiple pure-text benchmarks. Moreover, we provide in-depth analysis to demonstrate and validate the impact of modality collaboration through our proposed modality-adaptive module, especially in enhancing text tasks, including understanding, knowledge, and reasoning. Finally, comprehensive ablation studies validate the effectiveness of the proposed MLLM training paradigm, which can help inspire the development of future multimodal foundation models.

GPT-3 [6], LLaMA [57, 58], GPT-4 [46]와 같은 대규모 언어 모델(LLM)은 텍스트 이해 및 생성에서 뛰어난 일반화 능력으로 인해 큰 주목을 받고 있습니다. 비전 언어 응용을 촉진하기 위해 최근 GPT-4V1 [45]은 설명, 질문 답변 등 다양한 작업에서 인상적인 다중 모드 기능을 입증하여 비전 언어 분야의 잠재적 융합에 대한 연구자들의 관심을 불러 일으켰습니다. 이로 인해 시각 문제를 이해하고 처리하는 능력으로 LLM을 향상시키는 것을 목표로 하는 다중 모드 대규모 언어 모델(MLLM) 그룹[5, 15, 31, 38, 65, 66, 68, 75]이 등장하게 되었습니다[5, 15, 31, 38, 65, 66, 68, 75].

다중 모달 학습에 대한 이전 연구[27, 63]에 따르면 서로 다른 모달이 효과적으로 협업하여 텍스트 작업과 다중 모달 작업의 성능을 동시에 향상시킬 수 있다고 합니다. 그러나 MLLM은 특정 작업에 대한 미세 조정 없이 다양한 모달리티와 작업을 지원하는 통합 모델입니다. 최근 연구에서는 크로스 모달 정렬 모듈(예: Q-former [15, 31, 75] 및 리니어 레이어 [10, 38])을 사용하여 비전 인코더의 시각적 특징을 고정된 LLM에 매핑하여 보존된 언어 기능을 활용하여 멀티 모달 작업을 수행합니다. 안타깝게도 이 전략은 모달리티 협업의 잠재력을 제한합니다. 그 결과 일부 연구자[38, 68]는 멀티 모달 인스트럭션 튜닝 중에 LLM을 미세 조정하는 방법을 선택하기도 합니다. 미세 조정은 멀티 모달 작업을 크게 개선하지만 텍스트 작업 성능을 약화시킬 위험이 있습니다[16]. 그림 1에서 볼 수 있듯이 MLLM에서 모달리티 협업의 과제는 단일 모듈을 적용하여 모달리티 협업의 이득과 모달리티 간섭의 균형을 맞추는 것인데, 여러 모달리티의 수많은 인스트럭션 데이터세트에서 모달리티가 서로 간섭할 수 있기 때문입니다.

이러한 문제를 완화하기 위해 이번 연구에서는 새로운 범용 멀티 모달 기반 모델인 mPLUG-Owl2를 소개합니다. 이 모델은 멀티 모달 신호 관리를 위한 범용 인터페이스로 언어 디코더를 사용하여 모달 협업과 모달 간섭을 모두 고려하는 모듈화된 네트워크 설계를 특징으로 합니다. 특히 mPLUG-Owl2는 특정 공유 기능 모듈을 통합하여 모달리티 협업을 촉진하고 여러 모달리티에 걸쳐 피벗 역할을 하는 모달리티 적응형 모듈을 도입합니다. 따라서 시각 및 언어 모달리티는 교차 모달리티 상호 작용을 위해 공유 의미 공간에 투영되며, 제안된 모듈은 모달리티별 기능을 보존하는 데 도움이 됩니다. 새로운 아키텍처를 통해 다양한 정보 밀도를 가진 모달리티는 모달리티 적응형 모듈로 인해 모달리티 간섭으로부터 보호되며 공유 정보를 효과적으로 캡처하여 협업할 수 있습니다. 또한 시각 언어 사전 교육과 공동 시각 언어 교육 튜닝으로 구성된 혁신적인 2단계 교육 패러다임을 도입했습니다. 이 패러다임은 두 단계에 걸쳐 비전 인코더를 훈련시켜 저수준 및 고수준 의미론적 시각 정보를 보다 효과적으로 캡처할 수 있도록 합니다.

광범위한 실험을 통해 단일 일반 모델을 사용하여 8개의 고전적인 시각 언어 벤치마크에서 최첨단 성능을 달성한 mPLUG-Owl2의 효과와 일반화 능력을 확인할 수 있습니다. 또한 최근 5개의 제로샷 멀티모달 벤치마크에서 1위 또는 2위를 차지하여 멀티모달 명령어 이해 및 생성에 대한 적응성과 숙련도를 입증했습니다. 멀티 모달 작업에서의 최첨단 성능 외에도 mPLUG-Owl2는 여러 순수 텍스트 벤치마크에서도 최첨단 결과를 달성했습니다. 또한, 특히 이해, 지식, 추론 등 텍스트 작업을 향상시키는 데 있어 제안된 모달리티 적응형 모듈을 통해 모달리티 협업의 영향을 입증하고 검증하기 위한 심층 분석을 제공합니다. 마지막으로, 포괄적인 절제 연구를 통해 제안된 MLLM 교육 패러다임의 효과를 검증하여 향후 멀티모달 기초 모델 개발에 영감을 줄 수 있습니다.

## Related Work

### Multi-Modal Large Language Foundation Models

The successful application of Large Language Models (LLMs) has paved the way for developing several approaches aiming to augment the perceptual capacities of LLMs with additional modalities, all within a unified model. There are three primary methods for constructing multi-modal large language foundational models, each showing promise for robust zero-shot generalization capabilities in the visionlanguage domain. For instance, Flamingo [2] is a forerunner in this area, using a frozen vision encoder and a large language model equipped with gated cross-attention for crossmodality alignment. In contrast, PaLM-E [16] integrates extracted visual features directly through linear layers into the pre-trained PaLM [12] model, which boasts 520 billion parameters, thereby leading to robust performance across numerous real-world applications. This approach has been broadly adopted by models such as LLaVA [38], Shikra [10], etc. One significant limitation of this method, however, is the creation of lengthy visual sequences. To address this, BLIP-2 [31], drawing inspiration from DETR [8], developed a Q-former to reduce the sequence length of visual features efficiently. This design has been mirrored by Kosmos-1 [23], mPLUG-Owl [68], and MiniGPT-4 [75]. Nevertheless, it should be noted that these methods directly align the visual features with the LLMs, treating vision and language signals as equivalent, thereby overlooking the unique granularities between vision and language modalities. To alleviate this problem, we introduce modality-adaptive module. Our proposed model leads to superior performance in both zeroshot and fine-tuning evaluation settings in terms of both image and video.

대규모 언어 모델(LLM)의 성공적인 적용은 통합된 모델 내에서 추가 모달리티로 LLM의 지각 능력을 증강하는 것을 목표로 하는 여러 접근법을 개발하는 길을 열었습니다. 다중 모달 대규모 언어 기반 모델을 구축하는 세 가지 주요 방법이 있으며, 각 방법은 비전 언어 영역에서 강력한 제로 샷 일반화 기능을 제공할 수 있는 가능성을 보여줍니다. 예를 들어, 플라밍고[2]는 이 분야의 선구자로, 고정 비전 인코더와 교차 모달리티 정렬을 위한 게이트형 교차 주의 기능을 갖춘 대규모 언어 모델을 사용합니다. 이와는 대조적으로 PaLM-E[16]는 선형 레이어를 통해 추출된 시각적 특징을 5,200억 개의 파라미터를 자랑하는 사전 학습된 PaLM[12] 모델에 직접 통합하여 수많은 실제 애플리케이션에서 강력한 성능을 발휘합니다. 이 접근 방식은 LLaVA [38], Shikra [10] 등과 같은 모델에서 광범위하게 채택되었습니다. 그러나 이 방법의 한 가지 중요한 한계는 긴 시각적 시퀀스를 생성한다는 점입니다. 이 문제를 해결하기 위해 BLIP-2[31]는 DETR[8]에서 영감을 얻어 시각적 특징의 시퀀스 길이를 효율적으로 줄이기 위해 Q-포머를 개발했습니다. 이 설계는 Kosmos-1 [23], mPLUG-Owl [68], MiniGPT-4 [75]에 의해 반영되었습니다. 하지만 이러한 방법은 시각적 특징을 LLM에 직접 정렬하여 시각과 언어 신호를 동등한 것으로 취급하기 때문에 시각과 언어 양식 간의 고유한 세분성을 간과한다는 점에 유의해야 합니다. 이 문제를 완화하기 위해 모달리티 적응형 모듈을 도입했습니다. 우리가 제안한 모델은 이미지와 비디오 모두에서 제로 샷과 미세 조정 평가 설정 모두에서 우수한 성능을 제공합니다.

### Instruction Tuning with MLLMs

Instruction tuning optimizes pre-trained large language models to comprehend and adhere to natural instructions, thereby enhancing their ability to generalize unseen tasks in a zero-shot manner. Researchers often employ models such as ChatGPT and GPT-4 [46] to generate diverse and expansive instruction datasets, including those like Alpaca [56], ShareGPT [1], and WizardLM [61]. As multi-modal large language models emerge, research communities are beginning to create high-quality, diverse multi-modal datasets. For instance, MiniGPT-4 [75] utilizes GPT-3.5 to rephrase captions generated by pretrained models. Concurrently, LLaVA [38], SVIT [72], and LRV-Instruction [36] take advantage of image annotations, such as bounding boxes of objects, image captions, and region descriptions, to prompt GPT-4 to generate instructions and responses using self-instruction methods. Models such as mPLUG-Owl [68] and LLaVA-1.5 [37] further advance this area by undergoing joint training with language-only and vision-and-language instruction data, thereby mitigating the risk of catastrophic forgetting of language knowledge. Rather than merely preventing this phenomenon of catastrophic forgetting, mPLUG-Owl2, with the help of the modality-adaptive module, can gain from the collaborative efforts of modalities by being jointly trained with languageonly and multi-modal instruction data, thus enhancing both multi-modal and language-only performance.

명령어 튜닝은 사전 학습된 대규모 언어 모델을 최적화하여 자연스러운 명령어를 이해하고 준수함으로써 보이지 않는 작업을 제로 샷 방식으로 일반화할 수 있는 능력을 향상시킵니다. 연구자들은 종종 Alpaca [56], ShareGPT [1], WizardLM [61] 등의 모델을 포함하여 다양하고 광범위한 명령어 데이터 세트를 생성하기 위해 ChatGPT 및 GPT-4 [46]와 같은 모델을 사용합니다. 멀티모달 대규모 언어 모델이 등장함에 따라 연구 커뮤니티에서는 고품질의 다양한 멀티모달 데이터셋을 생성하기 시작했습니다. 예를 들어, MiniGPT-4[75]는 GPT-3.5를 활용하여 사전 학습된 모델에서 생성된 캡션을 다시 표현합니다. 동시에 LLaVA [38], SVIT [72], LRV-Instruction [36]은 물체의 경계 상자, 이미지 캡션, 영역 설명과 같은 이미지 주석을 활용하여 GPT-4가 자체 교육 방법을 사용하여 지침과 응답을 생성하도록 유도합니다. mPLUG-Owl [68] 및 LLaVA-1.5 [37]와 같은 모델은 언어 전용 및 시각과 언어 교육 데이터를 함께 훈련함으로써 이 영역을 더욱 발전시켜 언어 지식의 치명적인 망각 위험을 완화합니다. 단순히 이러한 치명적인 망각 현상을 방지하는 데 그치지 않고, 모달리티 적응 모듈의 도움을 받는 mPLUG-Owl2는 언어 전용 및 다중 모달 교육 데이터로 공동 훈련을 받음으로써 모달리티의 협력적인 노력을 통해 다중 모달 및 언어 전용 성능을 모두 향상시킬 수 있습니다.

## Methodology

### Overview

Figure 2 (a) sketches the overview of the mPLUG-Owl2. Specifically, our model comprises a vision encoder, a visual abstractor, a text embedding layer, and a language decoder. Notably, the standard implementation of the text embedding layer and language decoder involves the use of a large language model, such as GPT [6] or LLaMA [57]. We first briefly introduce our model’s architecture in Section 3.2. Furthermore, we handle different types of modalities by introducing the modality-adaptive module in Section 3.3. Lastly, we introduce the training paradigm for training mPLUG-Owl2 with modality collaboration in Section 3.4.

그림 2 (a)는 mPLUG-Owl2의 개요를 스케치한 것입니다. 구체적으로 이 모델은 비전 인코더, 시각 추상화기, 텍스트 임베딩 레이어, 언어 디코더로 구성되어 있습니다. 특히 텍스트 임베딩 레이어와 언어 디코더의 표준 구현에는 GPT [6] 또는 LLaMA [57]와 같은 대규모 언어 모델을 사용합니다. 먼저 3.2절에서 모델의 아키텍처를 간략하게 소개합니다. 또한 3.3절에서 양식 적응형 모듈을 소개하여 다양한 유형의 양식을 처리합니다. 마지막으로, 3.4절에서 모달리티 협업을 통해 mPLUG-Owl2를 훈련하기 위한 훈련 패러다임을 소개합니다.

### Model Architecture

As depicted in Figure 2, our model, referred to as mPLUGOwl2, is composed of three main components: a fundamental vision encoder [48], a visual abstractor, and a language decoder. Specifically, we utilize ViT-L/14 as the vision encoder and LLaMA-2-7B [58] as the language decoder. The vision encoder processes an input image with an H × W resolution and produces a sequence of H 14 × W 14 tokens. These visual token features are then combined with text token embeddings and fed into the language decoder that serves as a universal interface that converts various vision-language tasks into text-generation tasks. However, with the increase in image resolution, the encoded visual token sequences can exponentially lengthen. Additionally, the presence of abundant redundancy in the images (e.g., background, similar patches) leads to computational waste and introduces considerable noise. To address this, we propose a visual abstractor equipped with a fixed set of learnable queries to extract higher semantic features from images. Specifically, we feed the extracted visual token sequence I = [I1, I2, · · · , IP ] ∈ R P ×d and a fixed number of K learnable queries Q ∈ R K×d into the visual abstractor. Here, P = H 14 × W 14 represents the number of visual patches, and D is the hidden dimension. The visual abstractor consists of a series of visual abstractor layers. In the i-th layer of the visual abstractor, the compressed visual representations V i+1 are computed as follows:

(1)
(2)

Here, Attn(·, ·, ·) represents the self-attention operation, while W1 ∈ R d×d ′ and W2 ∈ R d ′×d are learnable parameters. The function SwiGLU(· · ·) refers to the SwiGLU activation function [51]. We designate V 0 = Q to initiate the process. Moreover, to augment the fine-grained perception ability, we integrate sinusoidal positional embeddings with the image feature I and V i , thereby preserving positional information, which has been proven essential in [8]. Hence, the computation required by the language decoder decreases from O((P + L) 2 ) to O((K + L) 2 ), significantly reducing computational load when P ≫ K, particularly in scenarios involving multiple images and when the text length L is relatively short. Once the compressed visual feature is obtained, it is concatenated with text token embeddings and then processed by the language decoder to generate the prediction.

그림 2에서 볼 수 있듯이, mPLUGOwl2라고 하는 저희 모델은 기본 비전 인코더[48], 시각 추상화기, 언어 디코더의 세 가지 주요 구성 요소로 이루어져 있습니다. 구체적으로, 비전 인코더로는 ViT-L/14를, 언어 디코더로는 LLaMA-2-7B[58]를 사용합니다. 비전 인코더는 H × W 해상도로 입력 이미지를 처리하여 H 14 × W 14 토큰의 시퀀스를 생성합니다. 이러한 시각 토큰 기능은 텍스트 토큰 임베딩과 결합되어 다양한 비전 언어 작업을 텍스트 생성 작업으로 변환하는 범용 인터페이스 역할을 하는 언어 디코더에 공급됩니다. 그러나 이미지 해상도가 증가함에 따라 인코딩된 시각 토큰 시퀀스는 기하급수적으로 길어질 수 있습니다. 또한 이미지에 풍부한 중복성(예: 배경, 유사한 패치)이 존재하면 계산 낭비가 발생하고 상당한 노이즈가 발생합니다. 이 문제를 해결하기 위해 이미지에서 더 높은 의미적 특징을 추출하기 위해 고정된 학습 가능한 쿼리 집합을 갖춘 시각적 추상화기를 제안합니다. 구체적으로, 추출된 시각적 토큰 시퀀스 I = [I1, I2, - - - , IP ] ∈ R P × d와 고정된 수의 K 개의 학습 가능한 쿼리 Q ∈ R K × d를 시각적 추상화에 공급합니다. 여기서 P = H 14 × W 14는 시각적 패치의 수를 나타내고 D는 숨겨진 차원입니다. 시각적 추상화기는 일련의 시각적 추상화기 레이어로 구성됩니다. 시각적 추상화의 i 번째 레이어에서 압축된 시각적 표현 V i+1은 다음과 같이 계산됩니다:

(1)
(2)

여기서 Attn(-, -, -)은 자기 주의 연산을 나타내고, W1 ∈ R d×d ′ 및 W2 ∈ R d ′×d는 학습 가능한 파라미터입니다. 함수 SwiGLU(- - -)는 SwiGLU 활성화 함수 [51]를 나타냅니다. 프로세스를 시작하기 위해 V 0 = Q를 지정합니다. 또한 세분화된 인식 능력을 강화하기 위해 정현파 위치 임베딩을 이미지 특징 I 및 V i와 통합하여 위치 정보를 보존하며, 이는 [8]에서 필수적인 것으로 입증되었습니다. 따라서 언어 디코더에 필요한 계산이 O((P + L) 2 )에서 O((K + L) 2 )로 감소하여 특히 여러 이미지가 포함된 시나리오와 텍스트 길이 L이 상대적으로 짧은 경우 P ≫ K일 때 계산 부하가 크게 줄어듭니다. 압축된 시각적 특징이 얻어지면 텍스트 토큰 임베딩과 연결한 다음 언어 디코더에서 처리하여 예측을 생성합니다.

### Modality-Adaptive Module

Prior approaches [15, 38, 68, 75] typically attempt to align visual features with language features by projecting image features into the language semantic space. However, this strategy can cause a mismatch in granularity , where image features often contain fruitful semantic information compared to the discrete semantic information within text embedding features. Those methods disregard the unique characteristics of visual and textual information, thus potentially limiting the model’s performance. To this end, we propose a new approach, namely, the Modality-Adaptive Module (MAM), which decouples vision-language representations by projecting visual features and language features into a shared semantic space while preserving the distinctive properties of each modality.

Formally, given a vision-language sequence X ∈ R (LV +LT )×d and modality indicators M ∈ {0, 1} (Lv+LT ) , we first define modality separated operation ϕ as:

(3)

where m ∈ {0, 1} is the type of modalities (i.e., vision or language). Given the previous layer’s output vectors Hl−1, l ∈ [1, L], where L is the number of language decoder layers, we first normalized different modalities into the same magnitude as follows:

(4)

where LNV and LNT are layer normalization [4] for visual features and language features respectively. Then, we reformulate the self-attention operation by leveraging separated linear projection layers for key projection matrix and value projection matrix while preserving query projection matrix shared as follows:

(5)
(6)
(7)
(8)

where W Q l , W K0 l , W K1 l , WV0 l , WV1 l ∈ R d×d are the learnable projection matrices, and Cl ∈ R (LV +LT )×d is the context features of l-th layer. In this manner, we can calculate the similarities between these two modalities within a shared semantic space, while also preserving the unique characteristics of each modality through different value projection layers. Moreover, by decoupling the key and value projection matrix, we can avoid interference between the two modalities, particularly in relation to granularity mismatch. In a similar vein, we also aim to model these characteristics by using different layer normalization layers. Finally, in order to promote modality collaboration within the same feature space, we maintain a shared FFN for both modalities. As a consequence, the model is able to preserve modality characteristics while achieving modality collaboration via the proposed modality-adaptive module.

기존의 접근 방식[15, 38, 68, 75]은 일반적으로 이미지 특징을 언어 의미 공간에 투영하여 시각적 특징을 언어 특징과 정렬하려고 시도합니다. 그러나 이 전략은 이미지 특징이 텍스트 임베딩 특징 내의 불연속적인 의미 정보에 비해 유익한 의미 정보를 포함하는 경우가 많아 세분성에서 불일치를 일으킬 수 있습니다. 이러한 방법은 시각 정보와 텍스트 정보의 고유한 특성을 무시하기 때문에 잠재적으로 모델의 성능을 제한할 수 있습니다. 이를 위해 각 양식의 고유한 특성을 유지하면서 시각적 특징과 언어 특징을 공유 의미 공간에 투영하여 시각-언어 표현을 분리하는 새로운 접근 방식, 즉 양식 적응형 모듈(Modality-Adaptive Module, MAM)을 제안합니다.
공식적으로 시각-언어 시퀀스 X ∈ R (LV +LT )×d와 양식 지표 M ∈ {0, 1} (Lv+LT )이 주어지면 먼저 양식 분리 연산 ϕ를 다음과 같이 정의합니다:

(3)

여기서 m ∈ {0, 1}은 모달리티의 유형(즉, 시각 또는 언어)입니다. 이전 계층의 출력 벡터 Hl-1, l ∈ [1, L]이 주어지면, 여기서 L은 언어 디코더 계층의 수이며, 먼저 다음과 같이 서로 다른 모달리티를 동일한 크기로 정규화합니다:

(4)

여기서 LNV와 LNT는 각각 시각적 특징과 언어 특징에 대한 레이어 정규화[4]입니다. 그런 다음 다음과 같이 공유되는 쿼리 투영 행렬을 유지하면서 키 투영 행렬과 값 투영 행렬에 대해 분리된 선형 투영 레이어를 활용하여 자기 주의 연산을 재구성합니다:

(5)
(6)
(7)
(8)

여기서 W Q l , W K0 l , W K1 l , WV0 l , WV1 l ∈ R d×d는 학습 가능한 투영 행렬이고, Cl ∈ R (LV +LT )×d는 l 번째 레이어의 컨텍스트 특징입니다. 이러한 방식으로 공유 의미 공간 내에서 두 양식 간의 유사성을 계산하는 동시에 서로 다른 값 투영 레이어를 통해 각 양식의 고유한 특성을 보존할 수 있습니다. 또한 키와 값 투영 행렬을 분리함으로써 특히 세분성 불일치와 관련하여 두 양식 간의 간섭을 피할 수 있습니다. 비슷한 맥락에서 서로 다른 레이어 정규화 레이어를 사용하여 이러한 특성을 모델링하는 것도 목표로 합니다. 마지막으로, 동일한 기능 공간 내에서 양식 간 협업을 촉진하기 위해 두 양식에 대해 공유 FFN을 유지합니다. 결과적으로 이 모델은 제안된 모달리티 적응형 모듈을 통해 모달리티 협업을 달성하면서 모달리티 특성을 보존할 수 있습니다.

### Training Paradigm

As depicted in Figure 2 (c), we employ a two-stage approach in training mPLUG-Owl2, comprising pre-training and visual instruction tuning similar to [38, 68], which aims to align the pre-trained vision encoder and language model during the pre-training phase, and then fine-tune the language model with language modeling loss during the instruction tuning phase. However, we find that simply freezing a pretrained vision encoder and training a vision-language projector to align visual data with language models can limit their capacity to interpret complex visual information, such as scene text and visual knowledge. To address the issue, we make the vision encoder trainable throughout both the pre-training and instruction tuning stages. This strategy allows the model to capture both low-level and high-level semantic visual information more effectively. Specifically, for the pre-training stage, we enable the vision encoder, visual abstractor, and a part of the modality-adaptive module to be trainable, while keeping the pre-trained language model frozen. Meanwhile, prior research in multi-modal learning [63] has indicated that significant enhancements can be achieved through the collaborative learning of unimodal and multi-modal sources. Based on this, we adopt a joint training approach by tuning the whole model during the instruction tuning stage, incorporating both text and multi-modal instructions. This methodology enhances the model’s comprehension of visual concepts embedded within the text by the multi-modal instructions. Concurrently, the text instruction data augments the model’s understanding of intricate natural instructions, thereby ensuring the preservation of its linguistic capabilities

그림 2 (c)에 표시된 바와 같이, 우리는 mPLUG-Owl2를 훈련할 때 [38, 68]과 유사한 사전 훈련과 시각적 명령어 튜닝으로 구성된 2단계 접근 방식을 사용하는데, 이는 사전 훈련 단계에서 사전 훈련된 비전 인코더와 언어 모델을 정렬한 다음 명령어 튜닝 단계에서 언어 모델링 손실로 언어 모델을 미세 조정하는 것을 목표로 합니다. 그러나 사전 학습된 비전 인코더를 고정하고 시각 데이터를 언어 모델에 맞추도록 비전 언어 프로젝터를 학습시키는 것만으로는 장면 텍스트나 시각 지식과 같은 복잡한 시각 정보를 해석하는 능력이 제한될 수 있다는 사실을 발견했습니다. 이 문제를 해결하기 위해 유니티는 사전 훈련과 인스트럭션 튜닝 단계 모두에서 비전 인코더를 훈련할 수 있도록 합니다. 이 전략을 통해 모델은 저수준 및 고수준 의미론적 시각 정보를 모두 더 효과적으로 캡처할 수 있습니다. 특히 사전 훈련 단계에서는 비전 인코더, 시각 추상화기, 양식 적응 모듈의 일부를 훈련할 수 있도록 하는 동시에 사전 훈련된 언어 모델은 고정된 상태로 유지합니다. 한편, 다중 모달 학습에 대한 선행 연구[63]에 따르면 단일 모달 및 다중 모달 소스의 공동 학습을 통해 상당한 향상을 달성할 수 있다고 합니다. 이를 바탕으로 당사는 지침 튜닝 단계에서 전체 모델을 튜닝하여 텍스트 및 멀티모달 지침을 모두 통합하는 공동 훈련 접근 방식을 채택했습니다. 이 방법론은 멀티모달 인스트럭션이 텍스트에 포함된 시각적 개념에 대한 모델의 이해도를 향상시킵니다. 동시에 텍스트 인스트럭션 데이터는 복잡한 자연어 인스트럭션에 대한 모델의 이해를 강화하여 언어적 능력을 보존합니다.

## Experiments

### Implementation

#### Data sets

mPLUG-Owl2 is first pre-trained on image-text pairs and fine-tunes on mono-modal and multi-modal instruction data. For pre-training data, we randomly pick about 400 million image-text pairs from five public datasets: Conceptual Captions (CC3M/CC12M) [9], COCO [35], Laionen [49], COYO [7], DataComp [18]. For instruction data, we collect 5 types of datasets including 1) image captioning (i.e., TextCaps [53], COCO [35]); 2) image question answering (i.e., VQAv2 [21], OKVQA [43], OCR-VQA [44], GQA [24], and A-OKVQA [50]); 3) region-aware QA (i.e., RefCOCO [69], VisualGenome [26]); 4) multi-modal instruct data (i.e., LLaVA-instruct-150K [38]); 5) text-only instruct data (i.e., ShareGPT-80K [1], SlimOrca [34]). Details can be found in the Appendix.

mPLUG-Owl2는 먼저 이미지-텍스트 쌍에 대해 사전 학습하고 모노 모달 및 멀티 모달 명령어 데이터에 대해 미세 조정합니다. 사전 학습 데이터의 경우, 5개의 공개 데이터 세트에서 약 4억 개의 이미지-텍스트 쌍을 무작위로 선택합니다: 개념적 캡션(CC3M/CC12M) [9], COCO [35], 라이오넨 [49], COYO [7], DataComp [18]. 인스트럭션 데이터의 경우, 1) 이미지 캡션(예: TextCaps [53], COCO [35]), 2) 이미지 질문 답변(예: VQAv2 [21], OKVQA [43], OCR-VQA [44], GQA [24], A-OKVQA [50]), 3) 지역 인식 QA(예:, RefCOCO [69], VisualGenome [26]), 4) 멀티 모달 인스트럭트 데이터(예: LLaVA-instruct-150K [38]), 5) 텍스트 전용 인스트럭트 데이터(예: ShareGPT-80K [1], SlimOrca [34]). 자세한 내용은 부록에서 확인할 수 있습니다.

#### Training Settings

We pre-train the model for 42,500 iterations with a batch size 8,192 for about 348 million imagetext pairs. Since we adopt the language modeling loss, the large batch size can be easily achieved by the gradient accumulation technique. mPLUG-Owl2 adopts ViT-L [48] with patch size 14 × 14 and pre-trained at resolution 224 × 224. We use the same data augmentation in BLIP2 [31], including random resized cropping, and horizontal flipping with a probability of 0.5. The number of layers in the visual abstractor is set to 6 and it is randomly initialized. The number of learnable queries is set to 64. For the language model, LLaMA-2 [58] is employed for handling multi-modal features with 7B parameters, and the parameters of modality-adaptive modules are initialized from the language model. We use the AdamW [40] optimizer with β1 = 0.9, β2 = 0.98 and ϵ =1e-6 for optimization. The cosine learning rate decay scheduler with a peak learning rate of 1e-4 and with warmup steps 1k. For the learning rate of the vision encoder, we employ layer-wise learning rate decay with a factor of 0.9 to retain the low-level visual representation. For the instruction tuning stage, we train the whole model for 1 epoch with a learning rate of 2e-5 and batch size 256. Besides, we increase the resolution from 224 × 224 to 448 × 448. The layer-wise learning rate decay is also employed which is crucial for retaining good visual representation in our experiments.

약 3억 4,800만 개의 이미지 텍스트 쌍에 대해 배치 크기 8,192개로 42,500회 반복하여 모델을 사전 훈련합니다. 언어 모델링 손실을 채택하기 때문에 그라데이션 누적 기법으로 큰 배치 크기를 쉽게 달성할 수 있습니다. mPLUG-Owl2는 패치 크기가 14×14이고 해상도 224×224로 사전 학습된 ViT-L [48]을 채택합니다. 무작위 크기 조정 자르기, 0.5 확률의 수평 뒤집기 등 BLIP2[31]에서와 동일한 데이터 증강을 사용합니다. 시각적 추상화기의 레이어 수는 6으로 설정되어 있으며 무작위로 초기화됩니다. 학습 가능한 쿼리 수는 64개로 설정됩니다. 언어 모델의 경우 7B 파라미터를 가진 다중 모달 기능을 처리하기 위해 LLaMA-2 [58]를 사용하며, 모달 적응형 모듈의 파라미터는 언어 모델에서 초기화됩니다. 최적화를 위해 β1 = 0.9, β2 = 0.98, ϵ =1e-6의 AdamW [40] 최적화기를 사용합니다. 최대 학습률이 1e-4이고 워밍업 단계가 1k인 코사인 학습률 감쇠 스케줄러를 사용합니다. 비전 인코더의 학습 속도는 낮은 수준의 시각적 표현을 유지하기 위해 계수 0.9의 레이어별 학습 속도 감쇠를 사용합니다. 명령어 튜닝 단계에서는 학습률 2e-5, 배치 크기 256으로 전체 모델을 1에포크 동안 훈련합니다. 또한 해상도를 224 × 224에서 448 × 448로 높입니다. 실험에서 좋은 시각적 표현을 유지하는 데 중요한 레이어별 학습 속도 감쇠도 사용합니다.


### Main Results

#### mage Caption and Visual Question Answering
